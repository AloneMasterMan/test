### 论文阅读：An Analysis of Scale Invariance in Object Detection – SNIP
#### 简介：检测方向，cvpr-2018 oral，作者所属机构：马里兰大学
#### 内容出处：https://zhuanlan.zhihu.com/p/36431183
#### 文章链接：https://arxiv.org/abs/1711.08189
#### 代码链接：http://bit.ly/2yXVg4c
Motavition：
在CNN中，我们需要不同种类的invariance（不变性）来做识别，这其中translation invariance在CNN中可以比较好地被考虑，然而另外一种重要的不变性，scale invariance就很难被CNN考虑到。

Contribution：
为了解决这个问题，一般常用的两大种策略就是Image Pyramid或者Feature Pyramid。在传统的Image Pyramid中，一般会使用一个固定大小的模板进行训练，将检测问题转换为一个固定大小的输入图像的分类问题。然后建立大小不同的Image Pyramid，在这个Pyramid中的每个scale的图片上，使用这个固定大小的检测器使用sliding window或cascaded classifier来分类。这大类方法中在deep时代比较经典的有MTCNN；另外一大类方法也就是Feature Pyramid，大家应该比较熟悉，这里的代表工作包括SPPNet和FPN，其中后者已经是目前检测算法的一个标准组件。

![image](https://github.com/GaoShanwen/test/blob/master/SNIP_analyze.jpeg)

  1、800(all)和1400(all)的对比：训练时使用不同大小的图训练，理论上如果使用更大图，小物体检测的性能应当有显著提升。但是实验表明这个提升非常小。文章中给出的解释是虽然1400的图训练会提升小物体的性能，但是会加大大物体训练的困难，所以此消彼长，并不会有比较大提升。（其实我是不太认可这个解释的，个人理解见下面的第三条。）
  
  2、1400(<80px)和1400(all)的对比：既然大物体太难train了，可能对小物体造成干扰，是否去掉大物体可以提升小物体的性能呢？答案也是否定的，而且损失掉的大物体的语义会让结果变得更加糟糕。
  
  3、MST：在Object Detection中，为了提升测试针对不同scale物体的性能，大家一般会使用Multi-scale training/testing这样的测试时融合的技巧来提升结果。与SNIP做法最大的区别就在于Multi-scale的做法扩充了不同scale样本的数目，但是仍然要求CNN去fit所有scale的物体。通过这样的一个对比实验，SNIP非常solid地证明了就算是数据相对充足的情况下，CNN仍然很难使用所有scale的物体。个人猜测由于CNN中没有对于scale invariant的结构，CNN能检测不同scale的“假象”，更多是通过CNN来通过capacity来强行memorize不同scale的物体来达到的，这其实浪费了大量的capacity，而SNIP这样只学习同样的scale可以保障有限的capacity用于学习语义信息。
所以，其实SNIP做的事情是非常简单的：在训练中，每次只回传那些大小在一个预先指定范围内的proposal的gradient，而忽略掉过大或者过小的proposal；在测试中，建立大小不同的Image Pyramid，在每张图上都运行这样一个detector，同样只保留那些大小在指定范围之内的输出结果，最终在一起NMS。这样就可以保证网络总是在同样scale的物体上训练，也就是标题中Scale Normalized的意思。
在加入snip后，得到了3.5个点的提升，加上DCN-RFCN得到7.4个点的提升；

